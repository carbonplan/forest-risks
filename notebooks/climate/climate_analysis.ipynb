{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import zarr\n",
    "import os\n",
    "import bokeh\n",
    "import dask\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from adlfs import AzureBlobFileSystem\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_gateway import Gateway\n",
    "\n",
    "gateway = Gateway()\n",
    "options = gateway.cluster_options()\n",
    "options.worker_memory = 24  # as high as 30 but you want tiny\n",
    "options.worker_cores = 2\n",
    "cluster = gateway.new_cluster(options)\n",
    "cluster.adapt(minimum=2, maximum=40)\n",
    "client = cluster.get_client()\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load in your list of models for which downscaled climate simulations are\n",
    "available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = AzureBlobFileSystem(\n",
    "    \"carbonplan\", account_key=os.environ[\"BLOB_ACCOUNT_KEY\"]\n",
    ")\n",
    "file_list = fs.ls(\"carbonplan-scratch/downscaling/bias-correction\")\n",
    "files = [file.split(\"/\")[-2] for file in file_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then load up a sample dataset to take a look at the domain and understand what\n",
    "you're working with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_url = f\"downscaling/bias-correction/{files[40]}\"\n",
    "store = zarr.storage.ABSStore(\n",
    "    \"carbonplan-scratch\",\n",
    "    prefix=store_url,\n",
    "    account_name=\"carbonplan\",\n",
    "    account_key=os.environ[\"BLOB_ACCOUNT_KEY\"],\n",
    ")\n",
    "sample_ds = xr.open_zarr(store, consolidated=True)\n",
    "sample_ds.pr.isel(time=0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can access the names of the individual GCMs that we have available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = list(set([(\".\").join(filename.split(\".\")[1:3]) for filename in files]))\n",
    "scenarios = [\n",
    "    (\"CMIP\", \"historical\", slice(\"1950\", \"2016\")),\n",
    "    (\"ScenarioMIP\", \"ssp245\", slice(\"2015\", \"2100\")),\n",
    "    (\"ScenarioMIP\", \"ssp370\", slice(\"2015\", \"2100\")),\n",
    "    (\"ScenarioMIP\", \"ssp585\", slice(\"2015\", \"2100\")),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a GIANT DATA CUBE because they're fun and they make any\n",
    "self-respecting pythonista feel proud. It'll also make the analysis way easier\n",
    "and more readable and cut down on loops (And who does loops anymore these days\n",
    "anyway? Certainly not _this_ gal...).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_cube(models, scenarios):\n",
    "    list_of_scenario_ds, list_of_scenario_coords = [], []\n",
    "    for (experiment, scenario, time_slice) in scenarios:\n",
    "        list_of_gcm_ds, list_of_gcm_coords = [], []\n",
    "        for gcm in models:\n",
    "            store_url = f\"downscaling/bias-correction/{experiment}.{gcm}.{scenario}.Amon.gn\"\n",
    "            # @joe how would i do this in an open_mfdataset kind of way?? < not released yet\n",
    "            store = zarr.storage.ABSStore(\n",
    "                \"carbonplan-scratch\",\n",
    "                prefix=store_url,\n",
    "                account_name=\"carbonplan\",\n",
    "                account_key=os.environ[\"BLOB_ACCOUNT_KEY\"],\n",
    "            )\n",
    "\n",
    "            if gcm in [\"HAMMOZ-Consortium.MPI-ESM-1-2-HAM\", \"NUIST.NESM3\"]:\n",
    "                # these keys break\n",
    "                continue\n",
    "            try:\n",
    "                ds = xr.open_zarr(store, consolidated=True).sel(time=time_slice)\n",
    "                ds[\"time\"] = pd.date_range(\n",
    "                    start=ds.indexes[\"time\"][0].strftime(\"%Y-%m\"),\n",
    "                    periods=ds.dims[\"time\"],\n",
    "                    freq=\"MS\",\n",
    "                )\n",
    "                ds = ds.drop(\n",
    "                    [\"height\", \"month\", \"member_id\", \"lat\", \"lon\"]\n",
    "                ).squeeze(drop=True)\n",
    "                list_of_gcm_ds.append(ds)\n",
    "                list_of_gcm_coords.append(gcm)\n",
    "            except:\n",
    "                print(store_url)\n",
    "        ds = xr.concat(\n",
    "            list_of_gcm_ds,\n",
    "            dim=xr.Variable(\"gcm\", list_of_gcm_coords),\n",
    "            coords=\"minimal\",\n",
    "            compat=\"override\",\n",
    "        )\n",
    "        list_of_scenario_ds.append(ds)\n",
    "        list_of_scenario_coords.append(scenario)\n",
    "\n",
    "    ds = xr.concat(\n",
    "        list_of_scenario_ds,\n",
    "        dim=xr.Variable(\"scenario\", list_of_scenario_coords),\n",
    "        coords=\"minimal\",\n",
    "        compat=\"override\",\n",
    "    )\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we open up each of the files into a list of datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_ds = create_data_cube(\n",
    "    models,\n",
    "    [\n",
    "        (\"ScenarioMIP\", \"ssp245\", slice(\"2015\", \"2100\")),\n",
    "        (\"ScenarioMIP\", \"ssp370\", slice(\"2015\", \"2100\")),\n",
    "        (\"ScenarioMIP\", \"ssp585\", slice(\"2015\", \"2100\")),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_ds = create_data_cube(\n",
    "    models, [(\"CMIP\", \"historical\", slice(\"1950\", \"2015\"))]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's mask out a few special regions of interest to get at regional\n",
    "responses. For now we'll just use some simple lat/lon bounding boxes but we\n",
    "could always expand to shapefiles for more refined analyses (e.g.\n",
    "states/counties/forest project regions).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_bounding_boxes = {\n",
    "    \"Pacific Northwest\": {\"lat\": [41, 49], \"lon\": [-130, -110]},\n",
    "    \"West\": {\"lat\": [20, 49], \"lon\": [-130, -105]},\n",
    "    \"Northeast\": {\"lat\": [41, 48], \"lon\": [-93, -66]},\n",
    "    \"Southeast\": {\"lat\": [25, 37], \"lon\": [-93, -76]},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(lat_lon_bounds_dict, ds):\n",
    "    \"\"\"\n",
    "    This will create a mask that aligns with your ds of interest. Requires ds to have coordinates of\n",
    "    lat and lon and also 'tasmax' as a hacky thing.\n",
    "    \"\"\"\n",
    "    lat_bounds, lon_bounds = (\n",
    "        lat_lon_bounds_dict[\"lat\"],\n",
    "        lat_lon_bounds_dict[\"lon\"],\n",
    "    )\n",
    "    ds = (\n",
    "        ds.where(ds.lat > lat_bounds[0])\n",
    "        .where(ds.lat < lat_bounds[1])\n",
    "        .where(ds.lon > lon_bounds[0])\n",
    "        .where(ds.lon < lon_bounds[1])\n",
    "    )\n",
    "    # hacky\n",
    "    mask = (\n",
    "        (\n",
    "            ds[\"tasmax\"]\n",
    "            .where(ds.lat > lat_bounds[0])\n",
    "            .where(ds.lat < lat_bounds[1])\n",
    "            .where(ds.lon > lon_bounds[0])\n",
    "            .where(ds.lon < lon_bounds[1])\n",
    "            .isel(time=0)\n",
    "            > 0\n",
    "        )\n",
    "        .drop([\"member_id\", \"month\", \"lat\", \"lon\", \"time\", \"height\"])\n",
    "        .squeeze()\n",
    "    )\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zarr_on_azure(filename, mode, ds=None, bucket=\"carbonplan-scratch\"):\n",
    "    \"\"\"This function either writes a ds to the desired bucket or\n",
    "    reads from a zarr file into a ds\"\"\"\n",
    "    store = zarr.storage.ABSStore(\n",
    "        bucket,\n",
    "        prefix=filename,\n",
    "        account_name=\"carbonplan\",\n",
    "        account_key=os.environ[\"BLOB_ACCOUNT_KEY\"],\n",
    "    )\n",
    "    if mode == \"w\":\n",
    "        ds.to_zarr(store, consolidated=True, mode=\"w\")\n",
    "    elif mode == \"r\":\n",
    "        ds = xr.open_zarr(store, consolidated=True)\n",
    "        return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recreate = False\n",
    "if recreate:\n",
    "    ds_dict = {\"historical\": historical_ds, \"future\": future_ds}\n",
    "    for (period, period_ds) in ds_dict.items():\n",
    "        for scenario in list(period_ds[\"scenario\"].values):\n",
    "            # breaking up scenario analyses to get around memory constraints\n",
    "            list_of_region_ds, list_of_region_coords = [], []\n",
    "            for (region, bounding_box) in tqdm(region_bounding_boxes.items()):\n",
    "                # we use the sample_ds we opened above to give it a sense of the array shape we're working with\n",
    "                mask = create_mask(region_bounding_boxes[region], sample_ds)\n",
    "                masked_ds = (\n",
    "                    period_ds.sel(scenario=scenario)\n",
    "                    .where(mask)\n",
    "                    .mean(dim=[\"x\", \"y\"])\n",
    "                    .load()\n",
    "                )\n",
    "                list_of_region_ds.append(masked_ds)\n",
    "                list_of_region_coords.append(region)\n",
    "            # then concatenate all of your masked things together\n",
    "            region_ds = xr.concat(\n",
    "                list_of_region_ds,\n",
    "                dim=xr.Variable(\"region\", list_of_region_coords),\n",
    "                coords=\"minimal\",\n",
    "                compat=\"override\",\n",
    "            )\n",
    "            print(\n",
    "                \"here we go! dask dask dask run run run for {}\".format(scenario)\n",
    "            )\n",
    "            region_ds = region_ds.load()\n",
    "            zarr_on_azure(\n",
    "                region_ds,\n",
    "                \"w\",\n",
    "                \"climate/regional_summaries_{}.zarr\".format(scenario),\n",
    "                bucket=\"carbonplan-scratch\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = [\"ssp245\", \"ssp370\", \"ssp585\"]\n",
    "ds_list = []\n",
    "\n",
    "for scenario in scenarios:\n",
    "    ds = zarr_on_azure(\n",
    "        \"climate/regional_summaries_{}.zarr\".format(scenario),\n",
    "        \"r\",\n",
    "        bucket=\"carbonplan-scratch\",\n",
    "    )\n",
    "    ds_list.append(ds)\n",
    "\n",
    "future_ds = xr.concat(\n",
    "    ds_list,\n",
    "    dim=xr.Variable(\"scenario\", scenarios),\n",
    "    coords=\"minimal\",\n",
    "    compat=\"override\",\n",
    ")\n",
    "\n",
    "historical_ds = (\n",
    "    zarr_on_azure(\n",
    "        \"climate/regional_summaries_{}.zarr\".format(\"historical\"),\n",
    "        \"r\",\n",
    "        bucket=\"carbonplan-scratch\",\n",
    "    )\n",
    "    .sel(scenario=\"historical\")\n",
    "    .drop(\"scenario\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decadal_dict = {\n",
    "    \"2020\": {\"time_slice\": slice(\"2015\", \"2024\")},\n",
    "    \"2050\": {\"time_slice\": slice(\"2045\", \"2054\")},\n",
    "    \"2080\": {\"time_slice\": slice(\"2075\", \"2084\")},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_summary(ds, aggregator, time_slice):\n",
    "    if aggregator == \"sum\":\n",
    "        return (\n",
    "            ds.sel(time=time_slice).groupby(\"time.year\").sum().mean(dim=\"year\")\n",
    "        )\n",
    "    elif aggregator == \"mean\":\n",
    "        return (\n",
    "            ds.sel(time=time_slice).groupby(\"time.year\").mean().mean(dim=\"year\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_delta(historical, future, method):\n",
    "    if method == \"absolute\":\n",
    "        return future - historical\n",
    "    if method == \"percentage\":\n",
    "        return (future - historical) / historical * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (period, time_dict) in decadal_dict.items():\n",
    "    historical_p = temporal_summary(\n",
    "        historical_ds[\"pr\"], \"sum\", slice(\"1985\", \"2014\")\n",
    "    )\n",
    "    historical_tasmax = temporal_summary(\n",
    "        historical_ds[\"tasmax\"], \"mean\", slice(\"1985\", \"2014\")\n",
    "    )\n",
    "    historical_tasmin = temporal_summary(\n",
    "        historical_ds[\"tasmin\"], \"mean\", slice(\"1985\", \"2014\")\n",
    "    )\n",
    "\n",
    "    future_p = temporal_summary(future_ds[\"pr\"], \"sum\", time_dict[\"time_slice\"])\n",
    "    future_tasmax = temporal_summary(\n",
    "        future_ds[\"tasmax\"], \"mean\", time_dict[\"time_slice\"]\n",
    "    )\n",
    "    future_tasmin = temporal_summary(\n",
    "        future_ds[\"tasmin\"], \"mean\", time_dict[\"time_slice\"]\n",
    "    )\n",
    "\n",
    "    decadal_dict[period][\"delta_p\"] = calculate_delta(\n",
    "        historical_p, future_p, \"percentage\"\n",
    "    )\n",
    "    decadal_dict[period][\"delta_tasmax\"] = calculate_delta(\n",
    "        historical_tasmax, future_tasmax, \"absolute\"\n",
    "    )\n",
    "    decadal_dict[period][\"delta_tasmin\"] = calculate_delta(\n",
    "        historical_tasmin, future_tasmin, \"absolute\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a nice color palette!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams[\"axes.prop_cycle\"] = mpl.cycler(color=[\"teal\", \"olive\", \"tomato\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have temperature and precip so we'll want to make an x-y scatter plot showing\n",
    "the changes in climate for each of our different climate simulations. We'll\n",
    "focus on the downscaled simulations since that's what is actually being fed into\n",
    "the subsequent drought/insect/fire models. While repeating these analyses for\n",
    "the raw vs. downscaled datasets would also be relevant, ideally the\n",
    "downscaling/bias-correction method should preserve the precip/temp deltas and so\n",
    "the difference between raw and downscaled deltas should be negligble.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in [\"Pacific Northwest\", \"West\", \"Northeast\", \"Southeast\"]:\n",
    "    fig, axarr = plt.subplots(ncols=3, figsize=(14, 5))\n",
    "    for (i, decade) in enumerate([\"2020\", \"2050\", \"2080\"]):\n",
    "        for scenario in scenarios:\n",
    "            ds_of_interest = decadal_dict[decade]\n",
    "\n",
    "            axarr[i].scatter(\n",
    "                x=ds_of_interest[\"delta_tasmax\"]\n",
    "                .sel(scenario=scenario, region=region)\n",
    "                .values,\n",
    "                y=ds_of_interest[\"delta_p\"]\n",
    "                .sel(scenario=scenario, region=region)\n",
    "                .values,\n",
    "                label=scenario,\n",
    "            )\n",
    "            for (gcm_num, gcm) in enumerate(\n",
    "                ds_of_interest[\"delta_p\"][\"gcm\"].values\n",
    "            ):\n",
    "                axarr[i].annotate(\n",
    "                    str(gcm_num + 1),\n",
    "                    (\n",
    "                        ds_of_interest[\"delta_tasmax\"]\n",
    "                        .sel(scenario=scenario, region=region, gcm=gcm)\n",
    "                        .values,\n",
    "                        ds_of_interest[\"delta_p\"]\n",
    "                        .sel(scenario=scenario, region=region, gcm=gcm)\n",
    "                        .values,\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "        axarr[i].axhline(y=0, color=\"grey\", lw=0.5)\n",
    "        axarr[i].axvline(x=0, color=\"grey\", lw=0.5)\n",
    "        axarr[i].set_xlim(-1, 7)\n",
    "        axarr[i].set_ylim(-10, 20)\n",
    "        axarr[i].set_xlabel(\"Change in temperature [$^\\circ$C]\")\n",
    "        axarr[i].set_ylabel(\"Change in precipitation [%]\")\n",
    "        axarr[i].set_title(decade, fontsize=16)\n",
    "        plt.legend()\n",
    "        plt.suptitle(region, fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "- region bounds map\n",
    "- 2015-2024, 2035-2044\n",
    "- gcm codes\n",
    "- show a little demo to talk about natural variability\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:notebook] *",
   "language": "python",
   "name": "conda-env-notebook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
